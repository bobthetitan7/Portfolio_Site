{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NWP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwfZRFsV07mYjb+odeo4o8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobthetitan7/Portfolio_Site/blob/main/NWP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv8R6bjL4JiI"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sumbLYJ5aDd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from absl import logging\n",
        "from google.colab import files\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "\n",
        "print(\"imported necessary libaries\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exB7GD706Wqf"
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NwBn7jRBgfr"
      },
      "source": [
        "# uplaod text document for testing\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWFqd6hArqnW"
      },
      "source": [
        "alice = 'Alice Adventure.txt'\n",
        "beyond = 'Beyond Good and Evil.txt'\n",
        "pride = 'Pride and Prejudice.txt'\n",
        "gatby = 'Gatsby.txt'\n",
        "cities = 'The Tale of two Cities.txt'\n",
        "\n",
        "alice = open(alice).read().lower()\n",
        "beyond = open(beyond).read().lower()\n",
        "pride = open(pride).read().lower()\n",
        "gatby = open(gatby).read().lower()\n",
        "cities = open(cities).read().lower()\n",
        "\n",
        "text = alice + beyond + pride + gatby + cities\n",
        "# you can change the text to lower ram usage\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93MilcP3s3Sc"
      },
      "source": [
        "text_parse = nltk.tokenize.sent_tokenize(text)\n",
        "words = []\n",
        "word_process = []\n",
        "for i in range(len(text_parse)):\n",
        "    text_parse[i] = text_parse[i].replace('\\n',' ')\n",
        "    text_parse[i] = text_parse[i].replace('  ',' ')\n",
        "    text_parse[i] = text_parse[i].replace('!','')\n",
        "    text_parse[i] = text_parse[i].replace('.','')\n",
        "    text_parse[i] = text_parse[i].replace(',','')\n",
        "    text_parse[i] = text_parse[i].replace('?','')\n",
        "    text_parse[i] = text_parse[i].replace(':','')\n",
        "    text_parse[i] = text_parse[i].replace(';','')\n",
        "    words_in_parse = text_parse[i].split(' ')\n",
        "    word_process.append(words_in_parse)\n",
        "    words = words + words_in_parse\n",
        "    \n",
        "\n",
        "\n",
        "sentence_process = text_parse\n",
        "test_case = []\n",
        "messages = []\n",
        "keys = []\n",
        "\n",
        "for i in range(len(sentence_process)):\n",
        "    while(len(word_process[i]) >= 4):\n",
        "        first_three = ' '.join(word_process[i][:3])\n",
        "        messages.append(first_three)\n",
        "        keys.append(word_process[i][3])\n",
        "        pairing = [first_three, word_process[i][3]]\n",
        "        del word_process[i][0]\n",
        "        test_case.append(pairing)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roI7VWu0iOXp"
      },
      "source": [
        "vocab_size = len(Counter(words)) + 1\n",
        "\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH66EQ2TkAXy"
      },
      "source": [
        "values = array(keys)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values) \n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJxTnvnVoQVk"
      },
      "source": [
        "onehot_encoded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIeiW9tpJOa5"
      },
      "source": [
        "encoding = embed(messages).numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9PTUAdTpOkI"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(encoding, onehot_encoded, test_size=0.30, random_state=42)\n",
        "\n",
        "encoding = 0\n",
        "onehot_encoded = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgZJFPU8wy-0"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BKZK7iSrORo"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUlvQ-9zrZhC"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8HaVLY_qYRc"
      },
      "source": [
        "X_train = X_train.reshape(30763, 1, 512)\n",
        "X_test = X_test.reshape(13185, 1, 512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJVUIVO7rsL5"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yqG4yJ55Z2U"
      },
      "source": [
        "def plot_curve(epochs, hist, metrics):\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "print(\"Loaded the plot_curve function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT9_0reV69nk"
      },
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_model(learning_rate):\n",
        "  \n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # 2 LSTM layer with a dropout of 0.2\n",
        "  model.add(LSTM(1024, dropout=0.2, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(l=0.00001)))\n",
        "\n",
        "  model.add(LSTM(1024, dropout=0.2, return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(l=0.00001)))\n",
        "\n",
        "  model.add(Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.00001)))\n",
        "  \n",
        "  \n",
        "  # softmax layer\n",
        "  model.add(tf.keras.layers.Dense(units=vocab_size, activation='softmax'))    \n",
        "\n",
        "                           \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNI8xU5GryaW"
      },
      "source": [
        "# def loss(labels, logits):\n",
        "#    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "#\n",
        "# def perplexity(labels, logits):\n",
        "#    cross_entropy = loss(labels, logits)\n",
        "#    perplexity = tf.keras.backend.exp(cross_entropy)\n",
        "#    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uckGnxkF_XWd"
      },
      "source": [
        "def train_model(model, x_train, y_train, epochs,\n",
        "                batch_size=None, validation_split=0.1):\n",
        "\n",
        "  history = model.fit(x_train, y_train, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True, \n",
        "                      validation_split=validation_split)\n",
        " \n",
        "\n",
        "  epochs = history.epoch\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "\n",
        "  return epochs, hist    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_quClK7_uU_"
      },
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 30\n",
        "batch_size = 1000\n",
        "\n",
        "validation_split = 0.2\n",
        "\n",
        "\n",
        "\n",
        "my_model = None\n",
        "\n",
        "my_model = create_model(learning_rate)\n",
        "\n",
        "epochs, hist = train_model(my_model, X_train, y_train, \n",
        "                           epochs, batch_size, validation_split)\n",
        "\n",
        "\n",
        "\n",
        "list_of_metrics_to_plot = ['categorical_accuracy']\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
        "\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "my_model.evaluate(X_test, y_test, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}